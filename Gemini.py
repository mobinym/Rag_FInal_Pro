# rag_system_v7_self_correcting_complete.py

import os
import re
import csv
import time
from datetime import datetime
from typing import List, Any
import numpy as np

# --- ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ ---
from langchain_community.document_loaders import Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document

# --- ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Retriever ---
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors.base import BaseDocumentCompressor
from sentence_transformers import CrossEncoder

# ==============================================================================
# Ø¨Ø®Ø´ Û±: ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾Ø±ÙˆÚ˜Ù‡
# ==============================================================================

DEBUG_MODE = True
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"
RERANKER_MODEL_NAME = "BAAI/bge-reranker-base"
LOCAL_LLM_NAME = "gemma3"

BASE_RETRIEVER_K = 10
FINAL_RETRIEVED_K = 3

CHUNK_SIZE = 300
CHUNK_OVERLAP = 60
DEFAULT_DOCUMENT_PATH = r"C:\path\to\your\document.docx" # Ù„Ø·ÙØ§ Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯
FEEDBACK_FILE_PATH = "feedback_logs/feedback.csv"

# ==============================================================================
# Ø¨Ø®Ø´ Û²: ØªØ¹Ø±ÛŒÙ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ùˆ ØªÙˆØ§Ø¨Ø¹ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ
# ==============================================================================

class RerankCompressor(BaseDocumentCompressor):
    """ÛŒÚ© ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø² Ø³Ù†Ø¯ Ø³ÙØ§Ø±Ø´ÛŒ Ú©Ù‡ Ø§Ø² ÛŒÚ© Cross-Encoder Ø¨Ø±Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¬Ø¯Ø¯ Ø§Ø³Ù†Ø§Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    class Config:
        arbitrary_types_allowed = True

    reranker: CrossEncoder
    top_n: int = FINAL_RETRIEVED_K

    def compress_documents(self, documents: List[Document], query: str, **kwargs: Any) -> List[Document]:
        if not documents:
            return []
        
        doc_list = [doc.page_content for doc in documents]
        pairs = [[query, doc] for doc in doc_list]
        
        scores = self.reranker.predict(pairs, show_progress_bar=False)
        
        docs_with_scores = list(zip(documents, scores))
        sorted_docs = sorted(docs_with_scores, key=lambda x: x[1], reverse=True)
        
        result = []
        for doc, score in sorted_docs[:self.top_n]:
            doc.metadata['relevance_score'] = score
            result.append(doc)
        return result

def load_and_clean_document(file_path: str) -> list[Document] | None:
    if not os.path.exists(file_path):
        print(f"Error: The file '{file_path}' was not found.")
        return None
    print("1. Loading and cleaning the document...")
    try:
        loader = Docx2txtLoader(file_path)
        documents = loader.load()
        documents[0].page_content = documents[0].page_content.strip()
        print("Document loaded and cleaned successfully.")
        return documents
    except Exception as e:
        print(f"An error occurred while loading or cleaning the file: {e}")
        return None

def save_feedback(report_data: dict, feedback: str, comment: str = "", correction: str = ""):
    os.makedirs(os.path.dirname(FEEDBACK_FILE_PATH), exist_ok=True)
    headers = [
        "timestamp", "question", "retrieved_context", "retrieval_scores",
        "raw_answer", "feedback_status", "user_comment", "corrected_answer",
        "retrieval_time_sec", "generation_time_sec"
    ]
    file_exists = os.path.isfile(FEEDBACK_FILE_PATH)
    try:
        with open(FEEDBACK_FILE_PATH, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            if not file_exists: writer.writerow(headers)
            scores = [f"{doc.metadata.get('relevance_score', 0):.4f}" for doc in report_data.get("retrieved_docs", [])]
            writer.writerow([
                datetime.now().isoformat(), report_data.get("question", ""),
                report_data.get("context_str", ""), ", ".join(scores),
                report_data.get("answer", ""), feedback, comment, correction,
                f"{report_data.get('retrieval_time', 0):.4f}", f"{report_data.get('generation_time', 0):.4f}"
            ])
        if feedback: print("Feedback saved successfully. Thank you!")
    except IOError as e:
        print(f"Error writing to feedback file: {e}")

# ==============================================================================
# Ø¨Ø®Ø´ Û³: Ù‡Ø³ØªÙ‡ Ø§ØµÙ„ÛŒ Ø³ÛŒØ³ØªÙ… RAG
# ==============================================================================

class RAGCore:
    def __init__(self, file_path: str):
        self.advanced_retriever = None
        self.llm = None
        self.prompt_template = None
        self._initialize_pipeline(file_path)

    def _initialize_pipeline(self, file_path: str):
        documents = load_and_clean_document(file_path)
        if not documents:
            raise FileNotFoundError(f"Document file not found at '{file_path}'.")
        
        print("2. Splitting the document into chunks...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
        chunks = text_splitter.split_documents(documents)

        print("3. Creating embeddings and vector store...")
        embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True})
        vector_store = FAISS.from_documents(chunks, embedding_model)
        print("Vector Store created successfully.")

        print("4. Setting up Advanced Retriever with Re-Ranking...")
        base_retriever = vector_store.as_retriever(search_kwargs={"k": BASE_RETRIEVER_K})
        
        reranker_model = CrossEncoder(RERANKER_MODEL_NAME, device='cpu')
        compressor = RerankCompressor(reranker=reranker_model, top_n=FINAL_RETRIEVED_K)
        
        self.advanced_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=base_retriever
        )
        print("Advanced Retriever is ready.")
        
        self.llm = OllamaLLM(model=LOCAL_LLM_NAME, temperature=0.0)
        
        prompt_str = """### Ù†Ù‚Ø´ ###
Ø´Ù…Ø§ ÛŒÚ© ØªØ­Ù„ÛŒÙ„â€ŒÚ¯Ø± Ø¯Ø§Ø¯Ù‡ Ù…ØªØ®ØµØµ Ùˆ ÛŒÚ© Ø­Ù‚ÛŒÙ‚Øªâ€ŒÛŒØ§Ø¨ (Fact-Checker) Ø¨Ø³ÛŒØ§Ø± Ø¯Ù‚ÛŒÙ‚ Ù‡Ø³ØªÛŒØ¯.
### Ù‡Ø¯Ù Ø§ØµÙ„ÛŒ ###
Ù‡Ø¯Ù Ø§ØµÙ„ÛŒ Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§ Ø¯Ù‚Øª ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø¨Ø§Ù„Ø§ Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ú©Ø§Ø±ØŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ù…ØªÙ† Â«Ø²Ù…ÛŒÙ†Ù‡Â» Ø±Ø§ Ø¨Ù‡ Ø¯Ù‚Øª Ø¯Ø±Ú© Ú©Ø±Ø¯Ù‡ØŒ Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª Ù…Ù†Ø·Ù‚ÛŒ Ø³Ø§Ø¯Ù‡ (Ù…Ø§Ù†Ù†Ø¯ Ù†Ø³Ø¨Øª Ø¯Ø§Ø¯Ù† Ø¶Ù…ÛŒØ± "Ù…Ù†" Ø¨Ù‡ Ú¯ÙˆÛŒÙ†Ø¯Ù‡ Ù…ØªÙ†) Ø±Ø§ Ø¨Ø±Ù‚Ø±Ø§Ø± Ú©Ù†ÛŒØŒ Ùˆ Ù¾Ø§Ø³Ø®ÛŒ Ø¯Ù‚ÛŒÙ‚ ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø²Ù…ÛŒÙ†Ù‡ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒ.
### Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§ØµÙ„ÛŒ (Ø¨Ø§ÛŒØ¯Ù‡Ø§) ###
1.  **Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ù‡ Ø¨Ù‡ Ú©Ù„Ù…Ù‡:** Ù¾Ø§Ø³Ø® Ø±Ø§ ØªØ§ Ø­Ø¯ Ø§Ù…Ú©Ø§Ù† Ú©Ù„Ù…Ù‡ Ø¨Ù‡ Ú©Ù„Ù…Ù‡ Ø§Ø² Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†.
2.  **ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ø²Ù…ÛŒÙ†Ù‡:** Ø¨Ù‡ ØªÙ…Ø§Ù… Ø¬Ø²Ø¦ÛŒØ§Øª (ÙØ§Ø¹Ù„â€ŒÙ‡Ø§ØŒ Ù…ÙØ¹ÙˆÙ„â€ŒÙ‡Ø§ØŒ ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ØŒ Ø§Ø¹Ø¯Ø§Ø¯ØŒ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ) Ø¯Ù‚Øª Ú©Ù†.
3.  **Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ùˆ Ø®Ù„Ø§ØµÙ‡:** ÙÙ‚Ø· Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ù¾Ø±Ø³ÛŒØ¯Ù‡ Ø´Ø¯Ù‡ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡. Ø§Ø² Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†.
4.  **Ø²Ø¨Ø§Ù† Ù¾Ø§Ø³Ø®:** Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø±Ø§ ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³.
### Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ (Ù†Ø¨Ø§ÛŒØ¯Ù‡Ø§) ###
1.  **Ù…Ù…Ù†ÙˆØ¹ÛŒØª Ø¯Ø§Ù†Ø´ Ø®Ø§Ø±Ø¬ÛŒ:** Ø§Ø² Ø¯Ø§Ù†Ø´ Ù‚Ø¨Ù„ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†.
2.  **Ù…Ù…Ù†ÙˆØ¹ÛŒØª Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù¾ÛŒÚ†ÛŒØ¯Ù‡:** Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø±Ø§ Ú©Ù‡ Ø¨Ù‡ ØµØ±Ø§Ø­Øª Ø¯Ø± Ù…ØªÙ† Ø¨ÛŒØ§Ù† Ù†Ø´Ø¯Ù‡ØŒ Ø­Ø¯Ø³ Ù†Ø²Ù†.
3.  **Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø§Ø³Ø® Ù†Ø§Ù…ÙˆØ¬ÙˆØ¯:** Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø¯Ø± Â«Ø²Ù…ÛŒÙ†Ù‡Â» ÛŒØ§ÙØª Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¨Ø§ÛŒØ¯ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø¨Ø§ Ø¹Ø¨Ø§Ø±Øª Â«Ù¾Ø§Ø³Ø® Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø¯Ø± Ø³Ù†Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª.Â» Ø¬ÙˆØ§Ø¨ Ø¯Ù‡ÛŒ.
---
Ø²Ù…ÛŒÙ†Ù‡:
{context}
---
Ø³ÙˆØ§Ù„:
{question}
---
Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…ØªÙ†:
"""
        self.prompt_template = PromptTemplate(template=prompt_str, input_variables=["context", "question"])
        print("5. LLM and Prompt are initialized.")

    def get_response_and_report(self, question: str) -> dict:
        start_time_retrieval = time.time()
        retrieved_docs = self.advanced_retriever.invoke(question)
        end_time_retrieval = time.time()
        
        context_str = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
        final_prompt = self.prompt_template.format(context=context_str, question=question)
        
        start_time_generation = time.time()
        answer = self.llm.invoke(final_prompt)
        end_time_generation = time.time()
        
        return {
            "question": question, "retrieved_docs": retrieved_docs, "context_str": context_str,
            "final_prompt": final_prompt, "answer": answer,
            "retrieval_time": end_time_retrieval - start_time_retrieval,
            "generation_time": end_time_generation - start_time_generation,
        }

    def self_correct_answer(self, report_data: dict) -> dict:
        """ÛŒÚ© Ù¾Ø§Ø³Ø® Ø±Ø§ Ú©Ù‡ ÙÛŒØ¯Ø¨Ú© 'Ø¨Ø¯' Ú¯Ø±ÙØªÙ‡ØŒ Ø¨Ø§Ø²Ø¨ÛŒÙ†ÛŒ Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ø§Ù…Ú©Ø§Ù† Ø§ØµÙ„Ø§Ø­ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        print("\nðŸ”„ Self-correction loop initiated...")
        context_str = report_data["context_str"]
        original_answer = report_data["answer"]

        critique_prompt_str = """Ù…ØªÙ† Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Â«Ø²Ù…ÛŒÙ†Ù‡Â» Ùˆ Ø§Ø¯Ø¹Ø§ÛŒ Ù¾Ø³ Ø§Ø² Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Â«Ø§Ø¯Ø¹Ø§Â» Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±.
Ø¢ÛŒØ§ Â«Ø§Ø¯Ø¹Ø§Â» Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ùˆ Ø¯Ù‚ÛŒÙ‚ ØªÙˆØ³Ø· Â«Ø²Ù…ÛŒÙ†Ù‡Â» Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ ÙÙ‚Ø· Ø¨Ø§ 'Ø¨Ù„Ù‡' ÛŒØ§ 'Ø®ÛŒØ±' Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡.

Ø²Ù…ÛŒÙ†Ù‡:
{context}
---
Ø§Ø¯Ø¹Ø§:
{claim}
---
Ù¾Ø§Ø³Ø® (Ø¨Ù„Ù‡/Ø®ÛŒØ±):
"""
        critique_prompt = PromptTemplate(template=critique_prompt_str, input_variables=["context", "claim"])
        critique_chain = critique_prompt | self.llm
        critique_result = critique_chain.invoke({"context": context_str, "claim": original_answer}).strip().lower()
        
        print(f"Self-critique result: LLM believes the answer was supported? -> '{critique_result}'")

        if "Ø®ÛŒØ±" in critique_result or "no" in critique_result:
            print("Diagnosis: Generation Failure detected. Attempting to re-generate...")
            correction_prompt = report_data["final_prompt"] + "\n\nÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ: Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ Ø´Ù…Ø§ ØªÙˆØ³Ø· Ø²Ù…ÛŒÙ†Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ø§ Ø¯Ù‚Øª Ø¨Ø³ÛŒØ§Ø± Ø¨ÛŒØ´ØªØ±ÛŒ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…ÛŒÙ†Ù‡ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯."
            new_answer = self.llm.invoke(correction_prompt)
            return {"corrected": True, "new_answer": new_answer, "reason": "Original answer was not supported by the context."}
        else:
            print("Diagnosis: Retrieval Failure suspected. Cannot improve with current context.")
            return {"corrected": False, "new_answer": "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø³Ù†Ø¯ØŒ Ù‚Ø§Ø¯Ø± Ø¨Ù‡ Ø§Ø±Ø§Ø¦Ù‡ Ù¾Ø§Ø³Ø® Ø¨Ù‡ØªØ±ÛŒ Ù†ÛŒØ³ØªÙ….", "reason": "The original answer seems correct based on the provided context, which itself might be irrelevant."}

# ==============================================================================
# Ø¨Ø®Ø´ Û´: Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
# ==============================================================================

def print_debug_report(report: dict):
    print("\n" + "="*25 + " DEBUG REPORT " + "="*25)
    print(f"\n[1. QUESTION]: {report['question']}")
    print(f"\n[2. ADVANCED RETRIEVER PERFORMANCE]:")
    print(f"   - Time taken: {report['retrieval_time']:.4f} seconds")
    print(f"   - Chunks retrieved and re-ranked: {len(report['retrieved_docs'])}")
    print("\n[3. FINAL RETRIEVED CHUNKS (Post Re-Ranking)]:")
    for i, doc in enumerate(report['retrieved_docs']):
        score = doc.metadata.get('relevance_score', 'N/A')
        score_str = f"{score:.4f}" if isinstance(score, float) else score
        print(f"   --- Chunk {i+1} (Re-Ranker Score: {score_str}) ---")
        indented_content = "      " + doc.page_content.replace("\n", "\n      ")
        print(indented_content)
    print(f"\n[4. LLM PERFORMANCE]:")
    print(f"   - Time taken: {report['generation_time']:.4f} seconds")
    print(f"\n[5. LLM FINAL ANSWER]:")
    print(f"   -> {report['answer']}")
    print("\n" + "="*24 + " END OF REPORT " + "="*25 + "\n")

def main():
    print("=" * 50)
    print("Welcome to the Intelligent Document Q&A System (v7.0 - Self-Correcting)")
    print("=" * 50)

    user_doc_path = input(f"Please enter the full path to the .docx file (or press Enter for default):\n[{DEFAULT_DOCUMENT_PATH}]\n> ")
    if not user_doc_path:
        user_doc_path = DEFAULT_DOCUMENT_PATH

    try:
        rag_system = RAGCore(file_path=user_doc_path)
    except Exception as e:
        print(f"\nAn unexpected error occurred during system startup: {e}")
        import traceback
        traceback.print_exc()
        return

    print("\nâœ… System is fully initialized and ready.")
    print("Type 'exit' or 'quit' to end the session.")
    print("-" * 50)

    while True:
        user_question = input("You: ")
        if user_question.lower() in ["exit", "quit", "Ø®Ø±ÙˆØ¬"]:
            print("Goodbye!")
            break
        
        report_data = rag_system.get_response_and_report(user_question)
        
        if DEBUG_MODE:
            print_debug_report(report_data)
        
        print("\nSystem's Answer:")
        print(report_data.get("answer", "No answer was received."))
        print("-" * 20)

        feedback_input = input("Was this answer helpful? (1: Good / 2: Bad / Enter: Skip)\n> ").lower()
        
        if feedback_input in ["2", "bad"]:
            print("\nI understand you weren't satisfied. Let me review and correct the answer...")
            correction_result = rag_system.self_correct_answer(report_data)
            
            print("\nâœ… Corrected Answer:")
            print(correction_result["new_answer"])
            print(f"(Reason for correction: {correction_result['reason']})")
            
            feedback_status = "bad_then_corrected" if correction_result["corrected"] else "bad_uncorrectable"
            save_feedback(report_data, feedback=feedback_status, correction=correction_result["new_answer"])

        elif feedback_input in ["1", "good"]:
            save_feedback(report_data, feedback="good")
        
        print("-" * 50)

if __name__ == "__main__":
    main()