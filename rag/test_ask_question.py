import time
import logging
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA
from rag.retriever import build_retriever
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer

# Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù„Ø§Ú¯Ø±
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)

# Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ ØªÙˆÚ©Ù† Ø®Ø±ÙˆØ¬ÛŒ
def get_token_count(text: str, model_name: str = "BAAI/bge-m3") -> int:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokens = tokenizer.tokenize(text)
    return len(tokens)

# Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø®
def run_query(qa_chain, query: str):
    logging.info(f"Query: {query}")
    start_time = time.time()

    result = qa_chain.invoke({"query": query})
    answer = result["result"]
    duration = round(time.time() - start_time, 2)

    logging.info(f"Answer: {answer}")
    logging.info(f"Token Count: {get_token_count(answer)}")
    logging.info(f"Duration: {duration} seconds")
    print("ðŸ§  Answer:", answer)
    print("-" * 50)

# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
def main():
    retriever = build_retriever()
    llm = OllamaLLM(model="gemma3")  # Ø§Ú¯Ø± Ø®Ø·Ø§ Ø¯Ø§Ø´ØªØŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† Ú©Ù‡ Ù…Ø¯Ù„ gemma3 Ù†ØµØ¨ Ø¨Ø§Ø´Ù‡

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        return_source_documents=True
    )

    while True:
        query = input("Enter your question (type 'exit' to quit):\nYou: ").strip()
        if query.lower() in {"exit", "quit"}:
            break
        run_query(qa_chain, query)

if __name__ == "__main__":
    main()
